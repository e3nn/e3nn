{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# TorchScript JIT Support\n",
    "\n",
    "PyTorch provides two ways to compile code into TorchScript: [tracing and scripting](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html). Tracing follows the tensor operations on an example input, allowing complex Python control flow if that control flow does not depend on the data itself. Scripting compiles a subset of Python directly into TorchScript, allowing data-dependent control flow but only limited Python features.\n",
    "\n",
    "This is a problem for e3nn, where many modules --- such as `TensorProduct` --- use significant Python control flow based on `Irreps` as well as features like inheritance that are incompatible with scripting. Other modules like `Gate`, however, contain important but simple data-dependent control flow. Thus `Gate` needs to be scripted, even though it contains a `TensorProduct` that has to be traced.\n",
    "\n",
    "To hide this complexity from the user and prevent difficult-to-understand errors, `e3nn` implements a wrapper for `torch.jit` --- [e3nn.util.jit](../api/util/jit.rst) --- that recursively and automatically compiles submodules according to directions they provide. Using the `@compile_mode` decorator, modules can indicate whether they should be scripted, traced, or left alone."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple Example: Scripting\n",
    "\n",
    "We define a simple module that includes data-dependent control flow:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from e3nn.o3 import Norm, Irreps\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, irreps_in):\n",
    "        super().__init__()\n",
    "        self.norm = Norm(irreps_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.norm(x)\n",
    "        if torch.any(norm > 7.):\n",
    "            return norm\n",
    "        else:\n",
    "            return norm * 0.5 \n",
    "\n",
    "irreps = Irreps(\"2x0e + 1x1o\")\n",
    "mod = MyModule(irreps)"
   ]
  },
  {
   "source": [
    "To compile it to TorchScript, we can try to use `torch.jit.script`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Compilation failed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mod_script = torch.jit.script(mod)\n",
    "except:\n",
    "    print(\"Compilation failed!\")"
   ]
  },
  {
   "source": [
    "This fails because `Norm` is a subclass of `TensorProduct` and TorchScript doesn't support inheritance. If we use `e3nn.util.jit.script`, on the other hand, it works:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.util.jit import script, trace\n",
    "mod_script = script(mod)"
   ]
  },
  {
   "source": [
    "Internally, `e3nn.util.jit.script` recurses through the submodules of `mod`, compiling each in accordance with its `@e3nn.util.jit.compile_mode` decorator if it has one. In particular, `Norm` and other `TensorProduct`s are marked with `@compile_mode('trace')`, so `e3nn.util.jit` constructs an example input for `mod.norm`, traces it, and replaces it with the traced TorchScript module. Then when the parent module `mod` is compiled inside `e3nn.util.jit.script` with `torch.jit.script`, the submodule `mod.norm` has already been compiled and is integrated without issue.\n",
    "\n",
    "As expected, the scripted module and the original give the same results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = irreps.randn(2, -1)\n",
    "assert torch.allclose(mod(x), mod_script(x))"
   ]
  },
  {
   "source": [
    "## Mixing Tracing and Scripting\n",
    "\n",
    "Say we define:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "@compile_mode('script')\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self, irreps_in):\n",
    "        super().__init__()\n",
    "        self.norm = Norm(irreps_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.norm(x)\n",
    "        for row in norm:\n",
    "            if torch.any(row > 0.1):\n",
    "                return row\n",
    "        return norm\n",
    "\n",
    "class AnotherModule(torch.nn.Module):\n",
    "    def __init__(self, irreps_in):\n",
    "        super().__init__()\n",
    "        self.mymod = MyModule(irreps_in)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mymod(x) + 3."
   ]
  },
  {
   "source": [
    "And trace an instance of `AnotherModule` using `e3nn.util.jit.trace`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = AnotherModule(irreps)\n",
    "example_inputs = (irreps.randn(3, -1),)\n",
    "mod2_traced = trace(\n",
    "    mod2,\n",
    "    example_inputs\n",
    ")"
   ]
  },
  {
   "source": [
    "Note that we marked `MyModule` with `@compile_mode('script')` because it contains control flow, and that the control flow is preserved even when called from the traced `AnotherModule`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[3., 3., 3.],\n        [3., 3., 3.]])\ntensor([4.1097, 3.2686, 4.9511])\n"
     ]
    }
   ],
   "source": [
    "print(mod2_traced(torch.zeros(2, irreps.dim)))\n",
    "print(mod2_traced(irreps.randn(3, -1)))"
   ]
  },
  {
   "source": [
    "We can confirm that the submodule `mymod` was compiled as a script, but that `mod2` was traced:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.jit._trace.TopLevelTracedModule'>\n<class 'torch.jit._script.RecursiveScriptModule'>\n"
     ]
    }
   ],
   "source": [
    "print(type(mod2_traced))\n",
    "print(type(mod2_traced.mymod))"
   ]
  },
  {
   "source": [
    "## Customizing Tracing Inputs\n",
    "\n",
    "Submodules can also be compiled automatically using tracing if they are marked with `@compile_mode('trace')`. When submodules are compiled by tracing it must be possible to generate plausible input examples on the fly.\n",
    "\n",
    "These example inputs can be generated automatically based on the `irreps_in` of the module (the specifics are the same as for `assert_equivariant`). If this is not possible or would yield incorrect results, a module can define a `_make_tracing_inputs` method that generates example inputs of correct shape and type."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "@compile_mode('trace')\n",
    "class TracingModule(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor, indexes: torch.LongTensor):\n",
    "        return x[indexes].sum()\n",
    "\n",
    "    # Because this module has no `irreps_in`, and because \n",
    "    # `irreps_in` can't describe indexes, since it's a LongTensor, \n",
    "    # we impliment _make_tracing_inputs\n",
    "    def _make_tracing_inputs(self, n: int):\n",
    "        import random\n",
    "        # The compiler asks for n example inputs --- \n",
    "        # this is only a suggestion, the only requirement \n",
    "        # is that at least one be returned.\n",
    "        return [\n",
    "            {\n",
    "                'forward': (\n",
    "                    torch.randn(5, random.randint(1, 3)), \n",
    "                    torch.arange(3)\n",
    "                )\n",
    "            }\n",
    "            for _ in range(n)\n",
    "        ]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 30,
   "outputs": []
  },
  {
   "source": [
    "To recursively compile this module and its submodules in accordance with their `@compile_mode`s, we can use `e3nn.util.jit.compile` directly. This can be useful if the module you are compiling is annotated with `@compile_mode` and you don't want to override that annotation by using `trace` or `script`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.jit._trace.TopLevelTracedModule'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from e3nn.util.jit import compile\n",
    "mod3 = TracingModule()\n",
    "mod3_traced = compile(mod3)\n",
    "print(type(mod3_traced))"
   ]
  },
  {
   "source": [
    "## Deciding between `@compile_mode('script')` and `@compile_mode('trace')`\n",
    "\n",
    "The easiest way to decide on a compile mode for your module is to try both. Tracing will usually generate warnings if it encounters dynamic control flow that it cannot fully capture, and scripting will raise compiler errors for features it does not support.\n",
    "\n",
    "In general, any module that uses inheritance or control flow based on `Irreps` in `forward()` will have to be traced."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Testing\n",
    "\n",
    "A helper function is provided to unit test that auto-JITable modules (those annotated with `@compile_mode`) can be compiled:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "AnotherModule(\n",
       "  original_name=AnotherModule\n",
       "  (mymod): RecursiveScriptModule(\n",
       "    original_name=MyModule\n",
       "    (norm): Norm(\n",
       "      original_name=Norm\n",
       "      (weight): ParameterDict(original_name=ParameterDict)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "from e3nn.util.test import assert_auto_jitable\n",
    "assert_auto_jitable(mod2)"
   ]
  },
  {
   "source": [
    "By default, `assert_auto_jitable` will test traced modules to confirm that they reject input shapes that are likely incorrect. Specifically, it changes `x.shape[-1]` on the assumption that the final dimension is a network architecture constant. If this heuristic is wrong for your module (like it is for `TracedModule` above), it can be disabled:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TracingModule(original_name=TracingModule)"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "assert_auto_jitable(mod3, strict_shapes=False)"
   ]
  }
 ]
}
